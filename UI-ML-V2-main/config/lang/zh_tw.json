{
    "font_path": "./resources/TaipeiSansTCBeta-Regular.ttf",

    "root": {
        "page_title": "AutoML"
    },
    "multi_app": {
        "menu_title": "åŠŸèƒ½"
    },
    "home": {
        "name": "Home-é¦–é ",
        "title": "MiTAC MiAutoML",
        "select_left": "â›¹ï¸â€â™‚ï¸ è«‹é¸æ“‡å·¦åˆ—é¸é …"
    },
    "eda3": {
        "name": "Data Profiling-è³‡æ–™è™•ç†",
        "title": "ğŸ“ˆğŸ“ŠğŸ“‰ MiAutoMLè³‡æ–™è™•ç† ğŸ“‰ğŸ“ŠğŸ“ˆ",
        "upload_here": "åœ¨é€™è£¡ä¸Šå‚³è³‡æ–™",
        "upload_help": "åªæ¥å— `csv` æª”",
        "select_sample_data": "è«‹é¸æ“‡ç¯„ä¾‹è³‡æ–™",
        "select_sample_help": "åœ¨é€™è£¡é¸æ“‡ç¯„ä¾‹è³‡æ–™",
        "hint1": "1.è«‹ç”¨å·¦å´ä¸Šå‚³åŠŸèƒ½ä¸Šå‚³ä¸€å€‹æª”æ¡ˆ",
        "show_data": "é¡¯ç¤ºè³‡æ–™",
        "uploaded_data": "æ‚¨ä¸Šå‚³çš„è³‡æ–™",
        "create_report": "ç”Ÿç”¢å ±å‘Š",
        "create_report_help": "é»æ“Šä»¥ç”Ÿç”¢å ±å‘Š\næ ¹æ“šæª”æ¡ˆå¤§å°å¯èƒ½æœƒèŠ±ä¸€é»æ™‚é–“",
        "generating_report": "â›© ç”¢ç”Ÿå ±å‘Š â›©",
        "hint2": "2.æˆ–é¸æ“‡ä¸€çµ„ç¯„ä¾‹è³‡æ–™"
    },
    "decision1": {
        "name": "Decision Tree-è¯æˆåŒ–æ±ºç­–æ¨¹",
        "title": "MiAutoMLæ±ºç­–æ¨¹ - Hyper Parameter Tuning",
        "selected_data": "### æ‰€é¸è³‡æ–™è¡¨",
        "upload_here": "åœ¨é€™è£¡ä¸Šå‚³è³‡æ–™",
        "upload_help": "åªæ¥å— `csv` æª”",
        "train_performance": "### Training Dataset Performance",
        "train_accuracy": "Acciracy: ",
        "confusion_matrix": "#### Confusion Matrix",
        "sensitivity": "Sensitivity / Recall: ",
        "specificity": "Specificity : ",
        "false_positive_rate": "False Positive Rate : ",
        "precision": "Precision / Positive Predictive Power : ",
        "negative_predictive_power": "Negative Predictive Power : ",
        "test_performance": "### Test Set Performance",
        "max_depth": "æœ€å¤§æ·±åº¦",
        "max_leaves": "æœ€å¤§è‘‰ç¯€é»æ•¸é‡",
        "min_samples_bf_split": "åˆ†æ”¯å‰æœ€å°‘æ¨£æœ¬æ•¸",
        "min_samples_in_leaf": "è‘‰ç¯€é»æœ€å°‘æ¨£æœ¬æ•¸",
        "split_criterion": "åˆ†æ”¯æ¢ä»¶",
        "decision_result": "### æ±ºç­–æ¨¹çµæœ"
    },
    "clf1": {
        "name": "Classification-åˆ†é¡",
        "title": "MiAutoML - Classification",
        "side_bar_hint": "è«‹ä¸Šå‚³æ‚¨çš„è³‡æ–™æˆ–é¸æ“‡ç¯„ä¾‹è³‡æ–™",
        "upload_here": "åœ¨é€™è£¡ä¸Šå‚³è³‡æ–™",
        "upload_help": "åªæ¥å— `csv` æª”",
        "select_sample_data": "è«‹é¸æ“‡ç¯„ä¾‹è³‡æ–™",
        "select_sample_help": "åœ¨é€™è£¡é¸æ“‡ç¯„ä¾‹è³‡æ–™",
        "show_data": "é¡¯ç¤ºè³‡æ–™",
        "current_data": "æ‚¨é¸æ“‡çš„è³‡æ–™ - ",
        "you_select": "æ‚¨é¸æ“‡äº†",
        "read_more": "é–±è®€æ›´å¤š",

        "penalty": "penalty",
        "penalty_help": "Used to specify the norm used in the penalization. The â€˜newton-cgâ€™, â€˜sagâ€™ and â€˜lbfgsâ€™ solvers support only l2 penalties. â€˜elasticnetâ€™ is only supported by the â€˜sagaâ€™ solver. If â€˜noneâ€™ (not supported by the liblinear solver), no regularization is applied.",
        "dual": "dual",
        "dual_help": "Dual or primal formulation. Dual formulation is only implemented for l2 penalty with liblinear solver. Prefer dual=False when n_samples > n_features.",
        "c": "C",
        "c_help": "Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.",
        "solver": "solver",
        "solver_help": "Algorithm to use in the optimization problem. for more read, visit documentation from below link",
        "multi_class": "multi_class",
        "multi_class_help": "If the option chosen is â€˜ovrâ€™, then a binary problem is fit for each label. For â€˜multinomialâ€™ the loss minimised is the multinomial loss fit across the entire probability distribution, even when the data is binary. â€˜multinomialâ€™ is unavailable when solver=â€™liblinearâ€™. â€˜autoâ€™ selects â€˜ovrâ€™ if the data is binary, or if solver=â€™liblinearâ€™, and otherwise selects â€˜multinomialâ€™.",

        "n_estimators": "n_estimators",
        "n_estimators_help": "The Number of tree in the forest. in africa's forest!!, ğŸ˜ğŸ˜.  Just kidding !!",
        "max_depth": "max_depth",
        "max_depth_help": "The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.",
        "criterion": "criterion",
        "criterion_help": "The function to measure the quality of a split. Supported criteria are â€œginiâ€ for the Gini impurity and â€œentropyâ€ for the information gain. Note: this parameter is tree-specific.",
        "min_samples_split": "min_samples_split",
        "min_samples_split_help": "The minimum number of samples required to split an internal node",
        "min_samples_leaf": "min_samples_leaf",
        "min_samples_leaf_help": "The minimum number of samples required to be at a leaf node",
        "max_features": "max_features",
        "max_features_help": "The number of features to consider when looking for the best split",
        "bootstrap": "bootstrap",
        "bootstrap_help": "Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree.",
        "max_samples": "max_samples",
        "max_samples_help": "If bootstrap is True, the number of samples to draw from X to train each base estimator.",

        "SVC_C": "SVC_C",
        "SVC_C_help": "Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive. The penalty is a squared l2 penalty.",
        "kernel": "kernel",
        "kernel_help": "Specifies the kernel type to be used in the algorithm. It must be one of â€˜linearâ€™, â€˜polyâ€™, â€˜rbfâ€™, â€˜sigmoidâ€™, â€˜precomputedâ€™ or a callable. If none is given, â€˜rbfâ€™ will be used., Missing `precomputed`",
        "degree": "degree",
        "degree_help": "Degree of the polynomial kernel function (â€˜polyâ€™). Ignored by all other kernels.",
        "coefo": "coef0",
        "coefo_help": "Independent term in kernel function. It is only significant in â€˜polyâ€™ and â€˜sigmoidâ€™.",

        "n_neighbours": "n_neighbours",
        "n_neighbours_help": "Number of neighbors to use by default for kneighbors queries.",
        "weights": "weights",
        "weights_help": "missing `callable`",
        "algorithm": "algorithm",
        "algorithm_help": "Algorithm used to compute the nearest neighbors",
        "leaf_size": "leaf_size",
        "leaf_size_help": "Leaf size passed to BallTree or KDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem. default: 30",
        "p": "p",
        "p_help": "Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2.",
        
        "run_model_error": "run model",

        "select_col": "è«‹é¸æ“‡ç›®æ¨™æ¬„ä½",
        "select_algo": "è«‹é¸æ“‡æ¼”ç®—æ³•",
        "select_algo_help": "åœ¨é€™é¸æ“‡æ¼”ç®—æ³•",
        "apply_preprocessing": "ä½¿ç”¨é è™•ç†",
        "apply_preprocessing_help": "If your data have some `object` type or `category` type columns then you have to `check` this box, because the machine learning model can not work with categorical values.   \nAfter clicking this box, there `OneHotEncoding` and `StanderdScaling` pipeline will create.  \n`NOTE:- ` Remember when you check this button that means you are downloading the full pipeline.   \nIf your data is clean and preprocessed then leave this checkbox `uncheck`",

        "training": "è¨“ç·´ä¸­ï¼Œè«‹ç¨ç­‰ ğŸ› ğŸ› ğŸ”§",
        "train_error": "è«‹é¸æ“‡ç›®æ¨™æ¬„ä½",
        "train": "é–‹å§‹è¨“ç·´",
        "train_help": "é»æ“Šä»¥é–‹å§‹è¨“ç·´",

        "f1_score": "f1 åˆ†æ•¸: ",
        "classification_report": "åˆ†é¡å ±å‘Š",
        "creat_conf_matrix": "ç”Ÿç”¢æ··æ·†çŸ©é™£ä¸­ ğŸ› ğŸ”§ğŸ› âš’",
        "can_download_link": "æ‚¨å¯ä»¥ç”¨ä¸‹æ–¹é€£çµä¸‹è¼‰è¨“ç·´æ¨¡å‹ ğŸˆ",
        "download_link": "ä¸‹è¼‰è¨“ç·´æ¨¡å‹ .pkl æª”",

        "upload_hint": "è«‹ä¸Šå‚³æ‚¨çš„è³‡æ–™æˆ–é¸æ“‡ç¯„ä¾‹è³‡æ–™"
    },
    "mlv2_V2": {
        "name": "Regression-å›æ­¸",
        "title": "MiAutoML",
        "side_bar_hint": "è«‹ä¸Šå‚³æ‚¨çš„è³‡æ–™æˆ–é¸æ“‡ç¯„ä¾‹è³‡æ–™",
        "upload_here": "åœ¨é€™è£¡ä¸Šå‚³è³‡æ–™",
        "upload_help": "åªæ¥å— `csv` æª”",
        "select_sample_data": "è«‹é¸æ“‡ç¯„ä¾‹è³‡æ–™",
        "select_sample_help": "åœ¨é€™è£¡é¸æ“‡ç¯„ä¾‹è³‡æ–™!!",
        "show_data": "é¡¯ç¤ºè³‡æ–™",
        "current_data": "æ‚¨é¸æ“‡çš„è³‡æ–™ - ",
        "you_select": "æ‚¨é¸æ“‡äº†",
        "read_more": "é–±è®€æ›´å¤š",

        "fit_intercept": "fit_intercept",
        "fit_intercept_help": "Whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (i.e. data is expected to be centered).",
        "normalize": "normalize",
        "normalize_help": "Want to use `normalize` or not",
        "n_jobs": "n_jobs",
        "n_jobs_help": "No of core will use to run the model, `-1` means all the core(much faster)",
        "lr_warning": "Select Relevant Hyperparameter you are seeing this message because bad hyperparameters are used, try to read about it from above link     \nMaybe your data is in bad shape!! you need yo apply `Preprocessing checkbox`",

        "n_estimator": "n_estimator",
        "n_estimator_help": "The number of trees in the forest. e.g. Numbers of Decision Trees, Higher number leads high variance model(overfit)",
        "criterion": "criterion",
        "criterion_help": "The function to measure the quality of a split. Supported criteria are for the mean squared error, which is equal to variance reduction as feature selection criterion, or the mean absolute error.",
        "max_depth": "max_depth",
        "max_depth_help": "The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples. higher value leads to high variance model(overfit).",
        "min_sample_split": "min_sample_split",
        "min_sample_split_help": "The minimum number of samples required to split an internal node or Decision trees",
        "min_sample_leaf": "min_sample_leaf",
        "min_sample_leaf_help": "The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.",
        "max_features": "max_features",
        "max_features_help": "The number of features to consider when looking for the best split",
        "bootstrap": "bootstrap",
        "bootstrap_help": "Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree.",
        "max_samples": "max_samples",
        "max_samples_help": "If bootstrap is True, the number of samples to draw from X to train each base estimator.",
        "rr_warning": "Selcet Relevant Hyperparameter (you are seeing this message because bad hyperparameters are used, try to read about it from above link)   \nMaybe your data is in bad shape!! you need yo apply `Preprocessing checkbox`",

        "kernal": "kernal",
        "kernal_help": "Specifies the kernel type to be used in the algorithm.",
        "degree": "degree",
        "degree_help": "Degree of the polynomial kernel function (Inored by all other kernels.",
        "gamma": "gamma",
        "gamma_help": "Kernel coefficient",
        "coef0": "coef0",
        "coef0_help": "Independent term in kernel function. ",
        "c": "C",
        "c_help": "Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive. The penalty is a squared l2 penalty.",
        "epsilon": "epsilon",
        "epsilon_help": "Epsilon in the epsilon-SVR model. It specifies the epsilon-tube within which no penalty is associated in the training loss function with points predicted within a distance epsilon from the actual value.",
        "shrinking": "shrinking",
        "shrinking_help": "Read from [here](https://scikit-learn.org/stable/modules/svm.html#shrinking-svm)",
        "max_iter": "max_iter",
        "max_iter_help": "Hard limit on iterations within solver, or -1 for no limit. set to -1 for initial",
        "svr_warning": "Selcet Relevant Hyperparameter. (you are seeing this message because bad hyperparameters are used, try to read about it from above link)    \nMaybe your data is in bad shape!! you need yo apply `Preprocessing checkbox`')  # handeling bad choose of hyperparameter",

        "n_neighbors": "n_neighbors",
        "n_neighbors_help": "Number of neighbors to use by default for kneighbors queries. more are [here](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor.kneighbors)",
        "algorithm": "algorithm",
        "algorithm_help": "Algorithm used to compute the nearest neighbors,'auto??will attempt to decide the most appropriate algorithm based on the values passed to fit method.'",
        "leaf_size": "leaf_size",
        "leaf_size_help": "Leaf size passed to BallTree or KDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem.",
        "p": "p",
        "p_help": "Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.",
        "kn_warning": "Select Relevant Hyperparameter (you are seeing this message because bad hyperparameters are used, try to read about it from above link)     \nMaybe your data is in bad shape!! you need yo apply `Preprocessing checkbox`",

        "select_col": "è«‹é¸æ“‡ç›®æ¨™æ¬„ä½",
        "select_col_help": "This list contain columns names form your data.    \nSelect the target column from here.!!",
        "select_algo": "è«‹é¸æ“‡æ¼”ç®—æ³•",
        "select_algo_help": "åœ¨é€™é¸æ“‡æ¼”ç®—æ³•",
        "select_col_warning": "è«‹å¾åˆ—è¡¨ä¸­é¸æ“‡ç›®æ¨™æ¬„ä½",
        "apply_preprocessing": "ä½¿ç”¨é è™•ç†",
        "apply_preprocessing_help": "If your data have some `object` type or `category` type columns then you have to `check` this box, because the machine learning model can not work with categorical values.   \nAfter clicking this box, there `OneHotEncoding` and `StanderdScaling` pipeline will create.  \n`NOTE:- ` Remember when you check this button that means you are downloading the full pipeline.   \nIf your data is clean and preprocessed then leave this checkbox `uncheck`",
        "train": "é–‹å§‹è¨“ç·´",
        "train_help": "é»æ“Šä»¥é–‹å§‹è¨“ç·´",
        "training": "è¨“ç·´ä¸­ï¼Œè«‹ç¨ç­‰ ğŸ› ğŸ› ğŸ”§",

        "train_score": "è¨“ç·´åˆ†æ•¸",
        "train_error": "è¨“ç·´èª¤å·®(RMSE)",
        "test_score": "æ¸¬è©¦åˆ†æ•¸",
        "test_error": "æ¸¬è©¦èª¤å·®(RMSE)",
        "train&test_plot": "è¨“ç·´èˆ‡æ¸¬è©¦é æ¸¬åœ–",

        "training_label": "Training Label",
        "training_predict": "Predicted on train data",
        "training_plot": "Training PLot",
        "test_label": "Test Data Label",
        "test_predict": "Predicted Data on test data",
        "test_plot": "Testing Plot",

        "training_residual": "Training Residual",
        "testing_residual": "Testing Residual",
        "can_download_link": "æ‚¨å¯ä»¥ç”¨ä¸‹æ–¹é€£çµä¸‹è¼‰è¨“ç·´æ¨¡å‹",
        "download_link": "ä¸‹è¼‰è¨“ç·´æ¨¡å‹ .pkl æª”",

        "upload_hint": "è«‹ä¸Šå‚³æ‚¨çš„è³‡æ–™æˆ–é¸æ“‡ç¯„ä¾‹è³‡æ–™"
    },
    "pno": {
        "name": "Predict Order-éŠ·å”®é æ¸¬",

        "upload_data_here": "åœ¨é€™è£¡ä¸Šå‚³è³‡æ–™",
        "upload_data_help":"åªæ¥å— `xlsx` æª”",
        "upload_data_hint": "è«‹ä¸Šå‚³æ‚¨çš„æ•¸æ“š",

        "upload_label_here": "åœ¨é€™è£¡ä¸Šå‚³å®¢æˆ¶æ¨™ç±¤",
        "upload_label_help":"åªæ¥å— `xlsx` æª”",
        "upload_label_hint": "è«‹ä¸Šå‚³æ‚¨çš„å®¢æˆ¶æ¨™ç±¤",
        
        "select_bu": "è«‹é¸æ“‡éƒ¨é–€",
        "select_label": "è«‹é¸æ“‡æ¨™ç±¤",

        "predict_period": "é æ¸¬é€±æœŸ(å¤©)",
        "submit": "é–‹å§‹é æ¸¬",
        "parsing": "é æ¸¬ä¸­...",

        "data_empty": "ç„¡ç›¸ç¬¦è³‡æ–™",
        "data_table": "è³‡æ–™è¡¨",
        "cus vs purchase": "å®¢æˆ¶åœ‹å®¶ vs è³¼è²·é‡",

        "actual_vs_predicted_plt": "{year}-{month} å¯¦éš› vs é æ¸¬",
        "predicti_plt": "{year}-{month} é æ¸¬",
        "download_prediction": "ä¸‹è¼‰",
        "saving_prediciton": "å„²å­˜ {filename}.xlsx ä¸­...",

        "ml_stat": "ML Stat",
        "r_squared": "R-squared (uncentered)",
        "adj_r_squared": "Adj. R-squared (uncentered)",
        "f_statistic": "F-statistic",
        "prob": "Prob (F-statistic)",
        "log_likeihood": "Log-Likeihood",
        "aic": "AIC",
        "bic": "BIC"
    },
    "pno2": {
        "name": "Predict Order-éŠ·å”®é æ¸¬",
        "train_configuration": "è¨“ç·´è¨­å®š",
        "training_month_count": "è¨“ç·´æœˆä»½æ•¸",
        "analyze_mode": "è¨“ç·´æ¨¡å¼",
        "start_analyze": "é–‹å§‹è¨“ç·´",
        "no_sc": "è«‹ä¸Šå‚³æ•¸æ“š",
        "loading_sc": "æ•¸æ“šä¸Šå‚³ä¸­...",
        "no_filter": "è«‹ä¸Šå‚³å®¢æˆ¶æ¨™ç±¤",
        "data_group_config": "æ•¸æ“šç¾¤çµ„è¨­å®š",
        "create_new_group": "æ–°å¢ç¾¤çµ„",
        "create_new_group_button": "æ–°å¢",
        "group_name": "ç¾¤çµ„åç¨±",
        "delete_group": "åˆªé™¤ç¾¤çµ„",
        "filter": "æ¨™ç±¤",
        "group_group": "åˆ†ç¾¤",
        "group_group_count": "åˆ†ç¾¤æ•¸é‡",
        "analyze_reuslt": "è¨“ç·´çµæœ",
        "average_customer": "æ¯æœˆå¹³å‡å®¢æˆ¶æ•¸é‡",
        "cross_compare": "äº¤å‰æ¯”å°çµæœ"
    },
    "ppo": {
        "name": "Prophet Predict Order"
    },
    "config": {
        "name": "Config-è¨­å®š",
        "title": "è¨­å®š",
        "language": "èªè¨€",
        "submit": "å„²å­˜"
    }
}